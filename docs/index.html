<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Structured Chemistry Reasoning with Large Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tangram.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Structured Chemistry Reasoning with Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ozyyshr.github.io">Siru Ouyang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://bcmi.sjtu.edu.cn/~zhangzs/">Zhuosheng Zhang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://bingyan.me/">Bing Yan</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XbtWYioAAAAJ&hl=en">Xuan Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a><sup>4,5</sup>,
            </span>
            <span class="author-block">
              <a href="https://hanj.cs.illinois.edu">Jiawei Han</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/lianhuiqin/home">Lianhui Qin</a><sup>5,6</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign, <sup>2</sup>Shanghai Jiao Tong University</span>
            <span class="author-block"><sup>3</sup>New York University, <sup>4</sup>University of Washington</span>
            <span class="author-block"><sup>5</sup>Allen Institute for AI, <sup>6</sup>University of California San-Diego</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2311.09656"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ozyyshr/StructChem"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://drive.google.com/file/d/1SuwRgV1LtEud8dfjftnw-zxBMgzSCwIT/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>ComposLoRA</span>
                </a>
              </span> -->
              <!-- Twitter Link. -->
              <!-- <span class="link-block">
                <a href="https://x.com/MingZhong_/status/1762347881812443575?s=20"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span></a>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <div class="content has-text-justified">
          This project introduce <strong>StructChem</strong>, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability.
          
          <video id="teaser" autoplay muted loop playsinline width="100%">
            <source src="./static/videos/intro.m4v"
                    type="video/mp4">
          </video>
          <h2 class="subtitle has-text-centered">
            Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. The errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality.
          </h2>
          <!-- <br><br><strong>Project Features</strong>: 
          <ul>
            <li><strong>üöÄ Training-Free Methods</strong>
              <ul>
              <li> LoRA Switch and LoRA Composite enable dynamic and precise integration of multiple LoRAs without fine-tuning.</li> 
              <li> Unlike methods that merge LoRA weights, ours focuses on the decoding process, keeping all LoRA weights intact.</li>
              </ul>
            </li> 
            <li><strong>üìä ComposLoRA Testbed</strong>
              <ul>
              <li> A new comprehensive platform, featuring 480 composition sets and 22 pre-trained LoRAs across six categories.</li> 
              <li> ComposLoRA is designed for the quantitative evaluation of LoRA-based composable image generation tasks.</li>
              </ul>
            </li>
            <li><strong>üìù GPT-4V-based Evaluator</strong>
              <ul>
              <li>We propose using GPT-4V as an evaluator to assess the efficacy of compositions and the quality of images.</li>
              <li>This evaluator has demonstrated a better correlation with human judgments.</li>
              </ul>
            </li>
            <li><strong>üèÜ Superior Performance</strong>
              <ul>
              <li>Both automated and human evaluations show that our approaches substantially outperform the prevalent LoRA Merge.</li>
              <li>Our methods exhibit a more significant advantage when generating complex compositions.</li>
              </ul>
            </li>
            <li><strong>üïµÔ∏è‚Äç‚ôÇÔ∏è Detailed Analysis</strong>
              <ul>
              <li>We delve deeply into the scenarios where each method excels.</li>
              <li>We explore the potential bias associated with using GPT-4V for evaluation.</li>
              </ul>
            </li>
          </ul>
         </div> -->
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="80%">
        <source src="./static/videos/method.m4v"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline width="100%">
        <source src="./static/videos/intro_video_2.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Multi-LoRA composition techniques effectively blend different elements into a cohesive image. Unlike the conventional LoRA Merge approach, which can lead to detail loss and image distortion as more LoRAs are added, our methods retain the accuracy of each element and the overall image quality.
      </h2>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Methodology</h2>
        <p align="center">
          <img src="./static/images/method_overview.png" class="center" width="100%">
          </p>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Formulae Generation:</strong>
              <ul>
              <li>Formulae serve as organized and abstracted representations of chemistry knowledge. When humans tackle intricate problems, the initial phase often involves seeking relevant knowledge as a foundation.</li> 
              <li>LLMs have indeed encoded much chemistry knowledge, it is often effective to elicit the knowledge from the parametric storage.</li>
              <li>We instruct the LLM not only to recite them but also to provide explanations for the variables they contain.</li>
              </ul>
            </li>
            <li><strong>Step-by-step Reasoning:</strong>
              <ul>
              <li>Grounded on the generated formulae, the LLMs can then reason about the solution to the original question.</li>
              <li>To induce LLMs for more precise reasoning and calculation processes, we adopt program-of-thoughts (PoT).</li>
              </ul>
            </li>

            <p align="center">
              <img src="./static/images/instruction_fr.png" class="center" width="100%">
              </p>

            <li><strong>Confidence-based Review-and-Refinement:</strong>
              <ul>
              <li>The generated formulae and step-by-step reasoning are not always error-free. The cumulative errors in the formulae generation or step-by-step reasoning process can amplify and propagate throughout the entire generation, leading to wrong answers.</li>
              <li>we estimate a confidence score on the revision process. Only a high-confidence revision is accepted for further refinement in the next iteration.itating the cohesive integration of all elements represented by different LoRAs.</li>
              </ul>
              <video id="teaser" autoplay muted loop playsinline width="100%">
                <source src="./static/videos/method.m4v"
                        type="video/mp4">
              </video>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Experimental Results</h2>
        <p align="center">
        <img src="./static/images/result.png" class="center" width="800%">
        </p>
        <div class="content has-text-justified">
          <ul>
            <li>Our proposed method consistently outperforms baseline methods by achieving an average of 30% improvement.</li>
            <li>StructChem works on both GPT-3,5 and GPT-4. The performance improvement on few-shot setting is even larger.</li>
            <li>StructChem achieves substantial performance gains in complex problems with extensive reasoning steps.</li>
          </ul>
        </div>
        <p align="center">
          <img src="./static/images/finetune.png" class="center" width="80%">
        </p>
        <div class="content has-text-justified">
          <ul>
            <li>Teach smaller open-sourced models how to reason: (1) Chemistry problems generated by GPT-4 as input; (2) Reasoning processes generated by StructChem as output </li>
            <li>StructChem achieves huge improvement over baselines, which validates the high quality of our generated reasoning process.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Analysis</h2>
        <p align="center">
        <img src="./static/images/ablation.png" class="center" width="70%">
        </p>
        <div class="content has-text-justified">
          <ul>
            <li>Both "structured instruction" and "iterative review and refinement" are significant in contributing to the performance of StructChem for zero-shot and few-shot settings.</li>
            <li>While iterative refinement indeed contributes to the performance, our strategy of structured instruction is strong enough and demonstrates comparative performance with strong baselines such as CoT.</li>
          </ul>
        </div>
        <p align="center">
          <img src="./static/images/switch_analysis.png" class="center" width="70%">
        </p>
        <div class="content has-text-justified">
          <ul>
            <li>The efficiency of the LoRA Switch improves progressively with increased step size, reaching peak performance at 5.</li>
            <li>The initial choice of LoRA in the activation sequence clearly influences overall performance, while alterations in the subsequent order have minimal impact.</li>
            <li>Though PoT helps with precise calculation and improves performance, StructChem without PoT still outperforms the strongest baselines.</li>
          </ul>
        </div>
        <p align="center">
          <img src="./static/images/error_cost.png" class="center" width="70%">
        </p>
        <div class="content has-text-justified">
          <ul>
            <li>StructChem are more likely to generate irrelevant formulae than inaccurate ones.</li>
            <li>Formulae being relevant probably is more important than being correct.</li>
            <li>Complex reasoning ability is still the bottleneck of LLMs.</li>
            <li>Preciseness is important for solving complex chemistry problems.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ouyang2023structured,
      title={Structured chemistry reasoning with large language models},
      author={Ouyang, Siru and Zhang, Zhuosheng and Yan, Bing and Liu, Xuan and Han, Jiawei and Qin, Lianhui},
      journal={arXiv preprint arXiv:2311.09656},
      year={2023}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-6">
        <div class="content">
          <p>
            The source code of this webpage is based on the <a href="https://github.com/nerfies/nerfies.github.io/">
              Nerfies</a> project webpage.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
